{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## V1: First version of the attack \n",
    "This version doesn't work as I don't handle mixed values right. E.g. columns like employment_since, stay same across cleaned and original datasets"
   ],
   "id": "a04bc3158e3f4471"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T13:07:55.299295Z",
     "start_time": "2025-04-24T13:07:50.391138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from dataset_uniqueness_utils import *\n",
    "\n",
    "DATA_DIR = \"datasets-p2\"\n",
    "OUT_DIR = \"output-datasets\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "unique_datasets, unique_names, num_unique = get_unique_datasets(DATA_DIR) \n",
    "\n",
    "print(f\"Using {num_unique} unique datasets:\")\n",
    "for name in unique_names:\n",
    "    print(f\" - {name}\")\n",
    "\n",
    "base_df = unique_datasets[0].copy()\n",
    "modification_log = []\n",
    "\n",
    "for row in range(base_df.shape[0]):\n",
    "    for col in base_df.columns:\n",
    "        values = [df.at[row, col] for df in unique_datasets]\n",
    "        try:\n",
    "            float_values = [float(v) for v in values]\n",
    "            is_numeric = True\n",
    "        except:\n",
    "            is_numeric = False\n",
    "\n",
    "        if is_numeric:\n",
    "            std_dev = np.std(float_values)\n",
    "            if std_dev < 1e-4:\n",
    "                cleaned_value = round(np.mean(float_values), 4)\n",
    "            else:\n",
    "                rounded = [round(v, 4) for v in float_values]\n",
    "                most_common = Counter(rounded).most_common(1)[0][0]\n",
    "                cleaned_value = most_common\n",
    "        else:\n",
    "            most_common = Counter(values).most_common(1)[0][0]\n",
    "            cleaned_value = most_common\n",
    "\n",
    "        if any(val != cleaned_value for val in values):\n",
    "            modification_log.append({\n",
    "                \"row\": row,\n",
    "                \"column\": col,\n",
    "                \"original_values\": values,\n",
    "                \"cleaned_value\": cleaned_value\n",
    "            })\n",
    "\n",
    "        base_df.at[row, col] = cleaned_value\n",
    "        \n",
    "save_cleaned_output_versioned(base_df, modification_log, num_unique)"
   ],
   "id": "f8e07a104bf99dd2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 5 unique datasets:\n",
      " - Financial_Records.csv\n",
      " - Financial_Records_Bob.csv\n",
      " - Financial_Records_Bob_Hedda_Fiedler.csv\n",
      " - Financial_Records_Bob_Nemanja_Saveski.csv\n",
      " - Financial_Records_Bob_Thomas_Senstyler.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 31\u001B[39m\n\u001B[32m     28\u001B[39m     is_numeric = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_numeric:\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m     std_dev = \u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfloat_values\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     32\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m std_dev < \u001B[32m1e-4\u001B[39m:\n\u001B[32m     33\u001B[39m         cleaned_value = \u001B[38;5;28mround\u001B[39m(np.mean(float_values), \u001B[32m4\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\TU_Wien\\Security_Privacy_and_Explainabiltiy_in_ML\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:4064\u001B[39m, in \u001B[36mstd\u001B[39m\u001B[34m(a, axis, dtype, out, ddof, keepdims, where, mean, correction)\u001B[39m\n\u001B[32m   4061\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   4062\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m std(axis=axis, dtype=dtype, out=out, ddof=ddof, **kwargs)\n\u001B[32m-> \u001B[39m\u001B[32m4064\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_methods\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_std\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mddof\u001B[49m\u001B[43m=\u001B[49m\u001B[43mddof\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4065\u001B[39m \u001B[43m                     \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\TU_Wien\\Security_Privacy_and_Explainabiltiy_in_ML\\.venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:223\u001B[39m, in \u001B[36m_std\u001B[39m\u001B[34m(a, axis, dtype, out, ddof, keepdims, where, mean)\u001B[39m\n\u001B[32m    221\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_std\u001B[39m(a, axis=\u001B[38;5;28;01mNone\u001B[39;00m, dtype=\u001B[38;5;28;01mNone\u001B[39;00m, out=\u001B[38;5;28;01mNone\u001B[39;00m, ddof=\u001B[32m0\u001B[39m, keepdims=\u001B[38;5;28;01mFalse\u001B[39;00m, *,\n\u001B[32m    222\u001B[39m          where=\u001B[38;5;28;01mTrue\u001B[39;00m, mean=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m--> \u001B[39m\u001B[32m223\u001B[39m     ret = \u001B[43m_var\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mddof\u001B[49m\u001B[43m=\u001B[49m\u001B[43mddof\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    224\u001B[39m \u001B[43m               \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhere\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwhere\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmean\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmean\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    226\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ret, mu.ndarray):\n\u001B[32m    227\u001B[39m         ret = um.sqrt(ret, out=ret)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\TU_Wien\\Security_Privacy_and_Explainabiltiy_in_ML\\.venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:214\u001B[39m, in \u001B[36m_var\u001B[39m\u001B[34m(a, axis, dtype, out, ddof, keepdims, where, mean)\u001B[39m\n\u001B[32m    211\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ret, mu.ndarray):\n\u001B[32m    212\u001B[39m     ret = um.true_divide(\n\u001B[32m    213\u001B[39m             ret, rcount, out=ret, casting=\u001B[33m'\u001B[39m\u001B[33munsafe\u001B[39m\u001B[33m'\u001B[39m, subok=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m--> \u001B[39m\u001B[32m214\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28;43mhasattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mret\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdtype\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m:\n\u001B[32m    215\u001B[39m     ret = ret.dtype.type(ret / rcount)\n\u001B[32m    216\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## V2: Versions works properly but has no noise\n",
    "This version addresses the issue from V1. Also stores data as integers (like in the original), and not float like V1"
   ],
   "id": "d4dbe90da5593ded"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T10:53:29.344131Z",
     "start_time": "2025-05-02T10:51:17.879756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "from dataset_uniqueness_utils import *\n",
    "\n",
    "def is_floatable(x):\n",
    "    try:\n",
    "        float(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "DATA_DIR = \"datasets-p2\"\n",
    "OUT_DIR = \"output-datasets\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "unique_datasets, unique_names, num_unique = get_unique_datasets(DATA_DIR)\n",
    "\n",
    "print(f\"Using {num_unique} unique datasets:\")\n",
    "for name in unique_names:\n",
    "    print(f\" - {name}\")\n",
    "\n",
    "base_df = unique_datasets[0].copy()\n",
    "modification_log = []\n",
    "\n",
    "for row in range(base_df.shape[0]):\n",
    "    for col in base_df.columns:\n",
    "        values = [df.at[row, col] for df in unique_datasets]\n",
    "\n",
    "        float_values = []\n",
    "        num_parseable = 0\n",
    "        for v in values:\n",
    "            try:\n",
    "                float_values.append(float(v))\n",
    "                num_parseable += 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if num_parseable == len(values):\n",
    "            std_dev = np.std(float_values)\n",
    "            if std_dev < 1e-4:\n",
    "                cleaned_value = round(np.mean(float_values), 4)\n",
    "            else:\n",
    "                rounded = [round(v, 4) for v in float_values]\n",
    "                most_common = Counter(rounded).most_common(1)[0][0]\n",
    "                cleaned_value = most_common\n",
    "\n",
    "        elif num_parseable == 0:\n",
    "            most_common = Counter(values).most_common(1)[0][0]\n",
    "            cleaned_value = most_common\n",
    "\n",
    "        else:\n",
    "            if random.random() < 0.5:\n",
    "                cleaned_value = round(np.mean(float_values), 4)\n",
    "            else:\n",
    "                cleaned_value = random.choice([v for v in values if not is_floatable(v)])\n",
    "\n",
    "        if any(val != cleaned_value for val in values):\n",
    "            modification_log.append({\n",
    "                \"row\": row,\n",
    "                \"column\": col,\n",
    "                \"original_values\": values,\n",
    "                \"cleaned_value\": cleaned_value\n",
    "            })\n",
    "\n",
    "        try:\n",
    "            base_df.at[row, col] = int(float(cleaned_value))\n",
    "        except:\n",
    "            base_df.at[row, col] = cleaned_value\n",
    "\n",
    "save_cleaned_output_versioned(base_df, modification_log, num_unique)"
   ],
   "id": "38316076467459c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 7 unique datasets:\n",
      " - Financial_Records.csv\n",
      " - Financial_Records_Bob.csv\n",
      " - Financial_Records_Bob_Cakmak_Dilara.csv\n",
      " - Financial_Records_Bob_Hedda_Fiedler.csv\n",
      " - Financial_Records_Bob_Lorenz_Horburger.csv\n",
      " - Financial_Records_Bob_Nemanja_Saveski.csv\n",
      " - Financial_Records_Bob_Thomas_Senstyler.csv\n",
      "Cleaned dataset saved as 'Financial_Records_No_Fingerprint_7_v1.csv' in 'output-datasets\\7_v1'\n",
      "Modification log saved as 'modification_log_7_v1.csv' in 'output-datasets\\7_v1'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output-datasets\\\\7_v1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## V3: Version with 0.5% noise",
   "id": "8d8b1fe9a26e3c18"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T19:30:53.915948Z",
     "start_time": "2025-05-03T19:06:05.512309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "from dataset_uniqueness_utils import *\n",
    "\n",
    "def is_floatable(x):\n",
    "    try:\n",
    "        float(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "DATA_DIR = \"datasets-p2\"\n",
    "OUT_DIR = \"output-datasets\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "unique_datasets, unique_names, num_unique = get_unique_datasets(DATA_DIR)\n",
    "\n",
    "print(f\"Using {num_unique} unique datasets:\")\n",
    "for name in unique_names:\n",
    "    print(f\" - {name}\")\n",
    "\n",
    "# Prepare a list of unique categorical values per column (from all datasets)\n",
    "all_possible_values = {}\n",
    "for col in unique_datasets[0].columns:\n",
    "    values = set()\n",
    "    for df in unique_datasets:\n",
    "        values.update(df[col].dropna().unique())\n",
    "    all_possible_values[col] = list(values)\n",
    "\n",
    "base_df = unique_datasets[0].copy()\n",
    "modification_log = []\n",
    "\n",
    "for row in range(base_df.shape[0]):\n",
    "    for col in base_df.columns:\n",
    "        values = [df.at[row, col] for df in unique_datasets]\n",
    "        all_identical = all(val == values[0] for val in values)\n",
    "\n",
    "        float_values = []\n",
    "        num_parseable = 0\n",
    "        for v in values:\n",
    "            try:\n",
    "                float_values.append(float(v))\n",
    "                num_parseable += 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # Fully categorical and identical\n",
    "        if num_parseable == 0 and all_identical and random.random() < 0.005:\n",
    "            # Introduce 0.5% noise by randomly picking a different category\n",
    "            original = values[0]\n",
    "            possible = [v for v in all_possible_values[col] if v != original]\n",
    "            if possible:\n",
    "                cleaned_value = random.choice(possible)\n",
    "                modification_log.append({\n",
    "                    \"row\": row,\n",
    "                    \"column\": col,\n",
    "                    \"original_values\": values,\n",
    "                    \"cleaned_value\": cleaned_value,\n",
    "                    \"note\": \"0.5% noise injected (categorical)\"\n",
    "                })\n",
    "            else:\n",
    "                cleaned_value = original  # fallback to original if no options\n",
    "        elif num_parseable == len(values):\n",
    "            # Fully numeric\n",
    "            std_dev = np.std(float_values)\n",
    "            if std_dev < 1e-4:\n",
    "                cleaned_value = round(np.mean(float_values), 4)\n",
    "            else:\n",
    "                rounded = [round(v, 4) for v in float_values]\n",
    "                most_common = Counter(rounded).most_common(1)[0][0]\n",
    "                cleaned_value = most_common\n",
    "        else:\n",
    "            # Mixed type\n",
    "            if random.random() < 0.005:\n",
    "                # 0.5% chance: override with a random categorical value\n",
    "                candidates = [v for v in values if not is_floatable(v)]\n",
    "                if candidates:\n",
    "                    cleaned_value = random.choice(candidates)\n",
    "                    modification_log.append({\n",
    "                        \"row\": row,\n",
    "                        \"column\": col,\n",
    "                        \"original_values\": values,\n",
    "                        \"cleaned_value\": cleaned_value,\n",
    "                        \"note\": \"0.5% noise injected (mixed â†’ categorical)\"\n",
    "                    })\n",
    "                elif float_values:\n",
    "                    cleaned_value = round(np.mean(float_values), 4)\n",
    "                else:\n",
    "                    cleaned_value = random.choice(values)\n",
    "            else:\n",
    "                if float_values:\n",
    "                    cleaned_value = round(np.mean(float_values), 4)\n",
    "                else:\n",
    "                    cleaned_value = random.choice(values)\n",
    "\n",
    "        # Log modifications (excluding noise already logged)\n",
    "        if all(val != cleaned_value for val in values):\n",
    "            if not any(\"note\" in entry and entry[\"row\"] == row and entry[\"column\"] == col for entry in modification_log):\n",
    "                modification_log.append({\n",
    "                    \"row\": row,\n",
    "                    \"column\": col,\n",
    "                    \"original_values\": values,\n",
    "                    \"cleaned_value\": cleaned_value\n",
    "                })\n",
    "\n",
    "        try:\n",
    "            base_df.at[row, col] = int(float(cleaned_value))\n",
    "        except:\n",
    "            base_df.at[row, col] = cleaned_value\n",
    "\n",
    "save_cleaned_output_versioned(base_df, modification_log, num_unique)"
   ],
   "id": "97569137cc02afa3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 7 unique datasets:\n",
      " - Financial_Records.csv\n",
      " - Financial_Records_Bob.csv\n",
      " - Financial_Records_Bob_Cakmak_Dilara.csv\n",
      " - Financial_Records_Bob_Hedda_Fiedler.csv\n",
      " - Financial_Records_Bob_Lorenz_Horburger.csv\n",
      " - Financial_Records_Bob_Nemanja_Saveski.csv\n",
      " - Financial_Records_Bob_Thomas_Senstyler.csv\n",
      "Cleaned dataset saved as 'Financial_Records_No_Fingerprint_7_v2.csv' in 'output-datasets\\7_v2'\n",
      "Modification log saved as 'modification_log_7_v2.csv' in 'output-datasets\\7_v2'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output-datasets\\\\7_v2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a627015b1945949"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
